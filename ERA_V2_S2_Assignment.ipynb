{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJptKBxALl-u",
        "outputId": "20aa77b5-bcbf-4437-b70a-6317032a76b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "import torch  # PyTorch library for deep learning\n",
        "import torch.nn as nn  # Module to create neural network layers\n",
        "import torch.nn.functional as F  # Functional interface containing typical operations used for building neural networks like activation functions\n",
        "import torch.optim as optim  # Optimizers for training neural networks\n",
        "from torchvision import datasets, transforms  # Datasets and transformations utilities from torchvision\n",
        "!pip install torchsummary  # Installing the torchsummary package for model summaries\n",
        "from torchsummary import summary  # Importing the summary function from torchsummary for detailed model summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()  # Check if CUDA (GPU support) is available\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")  # Select GPU if available, otherwise fall back to CPU\n",
        "device  # Display the selected device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Owi1LBNY8L",
        "outputId": "e28a8658-db9d-4c2f-8de4-b476f79d3682"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024  # Setting the batch size for training and testing\n",
        "\n",
        "# Loading and transforming the MNIST training dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))  # Normalize images\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)  # DataLoader with specified batch size and shuffling\n",
        "\n",
        "# Loading and transforming the MNIST testing dataset\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))  # Normalize images\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True)  # DataLoader with specified batch size and shuffling\n"
      ],
      "metadata": {
        "id": "EQZaZRGcNLtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e041d069-20f9-4b29-8b53-4509fd08f28a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 155808810.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 45797993.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 41298026.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21190799.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Notes on our naive model\n",
        "\n",
        "We are going to write a network based on what we have learnt so far.\n",
        "\n",
        "The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\"."
      ],
      "metadata": {
        "id": "r3gEjf-xMb-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nout ="
      ],
      "metadata": {
        "id": "WVSua2WWesHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6C7JJXNmesER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9PvBc0_oesBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstDNN(nn.Module): #the base class for all neural network modules in PyTorch.\n",
        "  def __init__(self): #The initializer method defines the layers and components of the neural network.\n",
        "    super(FirstDNN, self).__init__() #This line initializes the parent class (nn.Module), enabling the class to utilize all functionalities provided by PyTorch's nn.Module.\n",
        "    # r_in:1, n_in:28, j_in:1, s:1, r_out:3, n_out:28, j_out:1\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "    # r_in: 3, n_in:28 , j_in:1 , s:1 , r_out:5 , n_out: 28, j_out:1\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    # r_in:5 , n_in:28, j_in:1 , s:2 , r_out:6 , n_out:14 , j_out:2\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    # r_in:6 , n_in:14 , j_in:2 , s:1 , r_out:10 , n_out:14 , j_out:2\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "    # r_in:10 , n_in:14 , j_in:2 , s:1 , r_out:14 , n_out:14 , j_out:2\n",
        "    self.conv4 = nn.Conv2d(128, 256, 3, padding = 1)\n",
        "    # r_in:14 , n_in:14 , j_in:2 , s:2 , r_out: 16, n_out:7 , j_out:4\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    # r_in:16 , n_in:7 , j_in:4 , s:1 , r_out:24 , n_out: 5, j_out:4\n",
        "    self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "    # r_in:24 , n_in:5 , j_in:4 , s:1 , r_out:32 , n_out:3 , j_out:4\n",
        "    self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "    # r_in:32 , n_in:3 , j_in:4 , s:1 , r_out:40 , n_out: 1, j_out:4\n",
        "    self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "# Correct values\n",
        "# https://user-images.githubusercontent.com/498461/238034116-7db4cec0-7738-42df-8b67-afa971428d39.png\n",
        "  def forward(self, x):\n",
        "    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "    x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "    x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "    x = self.conv7(x)\n",
        "    x = F.relu(x) # Activation function applied to the output of the last convolutional layer\n",
        "    x = x.view(-1, 10)  # Flattening the output for the classification layer\n",
        "    return F.log_softmax(x)\n"
      ],
      "metadata": {
        "id": "Sir2LmSVLr_4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FirstDNN().to(device) #This operation ensures that the model will run on a GPU if one is available (and CUDA is enabled), otherwise, it will run on the CPU."
      ],
      "metadata": {
        "id": "sxICO4TTNt2H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(1, 28, 28)) #function then prints a detailed summary of the model, including the layers, their types, output shapes, and the number of parameters (both trainable and non-trainable). This is useful for understanding the architecture of the model, ensuring it is structured as intended, and estimating its computational complexity and memory usage."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M__MtFIYNwXa",
        "outputId": "ff1cc02e-d16e-4862-b3a4-3b9e606d70e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-fd10e37ac998>:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Sets the model in training mode. This is necessary because certain modules like Dropout and BatchNorm\n",
        "    # behave differently during training vs testing.\n",
        "    model.train()\n",
        "\n",
        "    # Wraps the train_loader with tqdm for a progress bar\n",
        "    pbar = tqdm(train_loader)\n",
        "\n",
        "    # Enumerates through all batches provided by train_loader\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        # Sends the data and target values to the device (CPU/GPU)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Clears the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: compute the model output given data\n",
        "        output = model(data)\n",
        "\n",
        "        # Compute the loss value\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the progress bar with the current loss value and the batch index\n",
        "        pbar.set_description(desc=f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    # Sets the model in evaluation mode. This is necessary because certain modules like Dropout and BatchNorm\n",
        "    # behave differently during training vs testing.\n",
        "    model.eval()\n",
        "\n",
        "    # Initializes the test loss and the number of correct predictions to 0\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Disables gradient calculation to save memory and computations, since it's not needed during testing\n",
        "    with torch.no_grad():\n",
        "        # Enumerates through all batches provided by test_loader\n",
        "        for data, target in test_loader:\n",
        "            # Sends the data and target values to the device (CPU/GPU)\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Forward pass: compute the model output given data\n",
        "            output = model(data)\n",
        "\n",
        "            # Accumulate the test loss by summing up losses of all batches\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "\n",
        "            # Get the index of the max log-probability as the prediction\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            # Counts the number of correct predictions\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Compute the average loss\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # Print the results for this epoch\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "ughX2C0uFe8x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Stochastic Gradient Descent (SGD) optimizer with specific parameters for the model.\n",
        "# model.parameters() provides all the trainable parameters of the model to the optimizer.\n",
        "# lr=0.01 sets the learning rate, which controls how much the weights of our model are adjusted with respect to the loss gradient.\n",
        "# momentum=0.9 helps in smoothing out the updates by considering the past gradients, which can accelerate training and reduce oscillations.\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Loop through epochs for training and testing. Here, it's set to run for 1 epoch as range(1, 2) generates a sequence that starts at 1 and stops before 2.\n",
        "for epoch in range(1, 2):\n",
        "    # Call the train function to train the model for one epoch.\n",
        "    # The function takes the model, device (CPU or GPU), training data loader, optimizer, and the current epoch index.\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # After training, call the test function to evaluate the model on the test dataset.\n",
        "    # This helps in monitoring the model's performance and generalization ability.\n",
        "    test(model, device, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0FYVWkGOFBS",
        "outputId": "2de608e2-f2d3-4cb6-af10-da63885fb52f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/59 [00:00<?, ?it/s]<ipython-input-6-fd10e37ac998>:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=2.177619457244873 batch_id=58: 100%|██████████| 59/59 [00:26<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1579, Accuracy: 3425/10000 (34%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer1 = optim.SGD(model.parameters(), lr=0.001, momentum=0.8)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer1, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "reIBU667OG_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa1467a-c600-4bcf-b8cb-de3e35d66462"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/59 [00:00<?, ?it/s]<ipython-input-6-fd10e37ac998>:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=2.302586078643799 batch_id=58: 100%|██████████| 59/59 [00:27<00:00,  2.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 980/10000 (10%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6agTEkqzz6TZ"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}